{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import timm\n",
    "import wandb\n",
    "import torch\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from torchvision.transforms import ToTensor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    classification_report,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Data for Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Data/TORGO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = Path(path)/f\"{Path(path).stem}_info.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z2/m5_kp65x4ydbtt3mq8bcpw_r0000gn/T/ipykernel_84868/1445064732.py:12: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(full_path)\n",
      "/opt/anaconda3/lib/python3.12/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Data/TORGO/F/F01/Session1/wav_headMic/0067.wav: \n",
      "Error processing Data/TORGO/F/F01/Session1/wav_headMic/0068.wav: \n"
     ]
    }
   ],
   "source": [
    "audio_file_count = 0\n",
    "audio_extensions = {\".mp3\", \".wav\", \".m4a\", \".aac\", \".flac\", \".ogg\", \".wma\"}\n",
    "file_info =[]\n",
    "# Traverse through the directory and subdirectories\n",
    "for root, dirs, files in os.walk(path):\n",
    "    for file in files:\n",
    "        file_extension = Path(file).suffix.lower()\n",
    "        if file_extension in audio_extensions:\n",
    "            try:\n",
    "                full_path = os.path.join(root, file)\n",
    "\n",
    "                y, sr = librosa.load(full_path)\n",
    "                directory_name = os.path.normpath(root).split(os.sep)\n",
    "                # Preprocess the info from the filename \n",
    "                \n",
    "                illness = 0 if (\"c\" in directory_name[-3][:2].lower()) else 1\n",
    "                gender = 'F' if (\"f\" in directory_name[-3][:2].lower()) else 'M'\n",
    "                file_info.append([full_path, illness, gender])\n",
    "            except Exception as e:\n",
    "                # Print the error message and the file path that caused the error\n",
    "                print(f\"Error processing {full_path}: {e}\")\n",
    "\n",
    "            # Preprocess the info from the filename \n",
    "            \n",
    "            \n",
    "data = pd.DataFrame(file_info, columns=[\"FileName\", \"Labels\", \"Gender\"])\n",
    "\n",
    "# Save DataFrame to a CSV file\n",
    "output_csv = output_path \n",
    "data.to_csv(output_csv, sep=',', header='true', index=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AudioDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        spectrogram = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            spectrogram = self.transform(spectrogram)\n",
    "        return spectrogram, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProcessData(d, batch_size= 512):\n",
    "    \n",
    "    data = []\n",
    "    labels = []\n",
    "    for _, row in tqdm(d.iterrows(), total=len(d), desc=\"Processing files\"):\n",
    "        # Get the file name/path from the DataFrame\n",
    "        file = row['FileName']  \n",
    "        \n",
    "        # Compute the Mel-spectrogram\n",
    "        #mels_db, sr = audio_to_melspectrograms(file)\n",
    "        mels_db, sr = audio_to_mfcc(file)\n",
    "        label_exploded = [row['Labels']] * mels_db.shape[0]\n",
    "        data.append(mels_db)\n",
    "        labels.append(label_exploded)\n",
    "    data = np.vstack(data)\n",
    "    labels = np.concatenate(labels)\n",
    "\n",
    "    # Standardize the data\n",
    "    mean = np.mean(data, axis=0)\n",
    "    std = np.std(data, axis=0)\n",
    "    epsilon = 1e-8  \n",
    "    data_standardized = (data - mean) / (std+epsilon)\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(data_standardized, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "    # Transform to Tensor\n",
    "    transform = ToTensor()\n",
    "\n",
    "    # Create the training and testing dataloader\n",
    "    train_dataset = AudioDataset(X_train, Y_train, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    test_dataset = AudioDataset(X_test, Y_test, transform=transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_to_mfcc(audio_file):\n",
    "    \"\"\"\n",
    "    Create the MFCC (Mel-Frequency Cepstral Coefficients) for the signal\n",
    "    \n",
    "    Args:\n",
    "        audio_file: The path to the audio file\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: MFCCs after normalizing\n",
    "        int: Sampling rate of the audio file\n",
    "    \"\"\"\n",
    "    y, sr = librosa.load(audio_file)\n",
    "    \n",
    "    # Frame the signal into short windows of 400ms and hop by 200ms\n",
    "    # So that each frame can have fixed length \n",
    "    frame_length = int(sr * 4)  \n",
    "    hop_length = int(sr * 2)   \n",
    "    \n",
    "    if len(y) > frame_length:\n",
    "        y_framed = librosa.util.frame(\n",
    "            y, frame_length=frame_length, hop_length=hop_length\n",
    "        )\n",
    "    else:\n",
    "        # If the signal is too short, pad it with zeros\n",
    "        y_padded = np.concatenate([y, np.zeros(frame_length - len(y))])\n",
    "        y_framed = librosa.util.frame(\n",
    "            y_padded, frame_length=frame_length, hop_length=hop_length\n",
    "        )\n",
    "    \n",
    "    # Compute MFCCs from the framed signal\n",
    "    mfccs = librosa.feature.mfcc(\n",
    "        y=y_framed.T, sr=sr, n_mfcc=40, hop_length=int(sr * 0.03), n_fft=512\n",
    "    )\n",
    "\n",
    "    \n",
    "    return mfccs, sr\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model and its relevant function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.model = timm.create_model(\"resnet18\", pretrained=True, num_classes=2)\n",
    "        self.model.conv1 = torch.nn.Conv2d(\n",
    "            1, self.model.conv1.out_channels,\n",
    "            kernel_size=self.model.conv1.kernel_size,\n",
    "            stride=self.model.conv1.stride,\n",
    "            padding=self.model.conv1.padding,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        # Set the loss function and optimizer\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "        # Set the device (CPU or GPU)\n",
    "        self.device = device\n",
    "\n",
    "    \n",
    "    def fit(self, train_loader, epochs):\n",
    "        history = {'loss':[], 'accuracy':[]}\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Set model to training mode\n",
    "            self.train()\n",
    "\n",
    "            print(\"\\nEpoch {}/{}\".format(epoch+1, epochs))\n",
    "\n",
    "            with tqdm(total=len(train_loader)) as pbar:\n",
    "                for _, batch in enumerate(train_loader):\n",
    "                    # Unpack the batch\n",
    "                    inputs, labels = batch  \n",
    "                    inputs = inputs.to(torch.float32).to(self.device)\n",
    "                    labels = labels.to(torch.long).to(self.device)\n",
    "\n",
    "                    # Zero the parameter gradients\n",
    "                    self.optimizer.zero_grad()\n",
    "\n",
    "                    \n",
    "                    outputs = self.model(inputs)\n",
    "                    loss = self.criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "\n",
    "                    # Update the parameters\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                    pbar.update(1)        \n",
    "            \n",
    "            # Model evaluation on train data\n",
    "            train_loss, train_report = self.evaluate(train_loader)\n",
    "            train_acc = train_report[0]\n",
    "            print(f\"loss: {train_loss:.4f} - accuracy: {100 *train_acc:.4f}%\")\n",
    "\n",
    "            # Log metrics to WandB\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"train_accuracy\": train_acc\n",
    "            })\n",
    "\n",
    "            # Store the model's training progress\n",
    "            history['loss'].append(train_loss)\n",
    "            history['accuracy'].append(train_acc)\n",
    "            \n",
    "        return history\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Set model to evaluation mode\n",
    "        self.eval()  \n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(X)\n",
    "        return outputs\n",
    "    \n",
    "    def evaluate(self, data_loader):\n",
    "        # Set model to evaluation mode\n",
    "        self.eval()\n",
    "\n",
    "        running_loss = torch.tensor(0.0).to(self.device)\n",
    "\n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "        report = []\n",
    "        \n",
    "        with tqdm(total=len(data_loader), desc=\"Evaluating\") as pbar:  # Add colon and length of data_loader\n",
    "        \n",
    "            for step, batch in enumerate(data_loader):\n",
    "                # Unpack the batch\n",
    "                inputs, labels = batch  \n",
    "                \n",
    "                inputs = inputs.to(torch.float32).to(self.device)\n",
    "                labels = labels.to(torch.long).to(self.device)\n",
    "                \n",
    "                outputs = self.predict(inputs)\n",
    "\n",
    "                # Compute batch loss\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                running_loss += loss\n",
    "\n",
    "                # Calculate batch accuracy\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                predictions.extend(predicted.cpu().detach().numpy())\n",
    "                true_labels.extend(labels.cpu().detach().numpy())\n",
    "\n",
    "                # Update the progress bar\n",
    "                pbar.update(1)\n",
    "        \n",
    "            loss = running_loss.item() / (step+1)\n",
    "            # Calculate metrics using accumulated true labels and predictions\n",
    "            report.append(accuracy_score(true_labels, predictions))\n",
    "            report.append(balanced_accuracy_score(true_labels, predictions))\n",
    "            report.append(classification_report(true_labels, predictions))\n",
    "            \n",
    "        \n",
    "        return loss, report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:sioka1dl) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eafdcf42973a4d3484f62488e38f27ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.009 MB of 0.009 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">spring-monkey-24</strong> at: <a href='https://wandb.ai/tinghui/speech_bias/runs/sioka1dl' target=\"_blank\">https://wandb.ai/tinghui/speech_bias/runs/sioka1dl</a><br/> View project at: <a href='https://wandb.ai/tinghui/speech_bias' target=\"_blank\">https://wandb.ai/tinghui/speech_bias</a><br/>Synced 4 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240926_235213-sioka1dl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:sioka1dl). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tinghui/workspace/github/Speech_dataset_practice/wandb/run-20240926_235225-b46peq3u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tinghui/speech_bias/runs/b46peq3u' target=\"_blank\">silvery-paper-25</a></strong> to <a href='https://wandb.ai/tinghui/speech_bias' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tinghui/speech_bias' target=\"_blank\">https://wandb.ai/tinghui/speech_bias</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tinghui/speech_bias/runs/b46peq3u' target=\"_blank\">https://wandb.ai/tinghui/speech_bias/runs/b46peq3u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 17631/17631 [01:12<00:00, 244.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 4/4 [00:31<00:00,  7.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-training accuracy: 48.8692%\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 5/16 [01:15<02:45, 15.09s/it]"
     ]
    }
   ],
   "source": [
    "def train(model, train_loader, test_loader):\n",
    "    \n",
    "    loss, score = model.evaluate(test_loader)\n",
    "    \n",
    "    wandb.log({\n",
    "        \"test_loss\": loss,\n",
    "        \"test_accuracy\": score[0]\n",
    "    })\n",
    "    print(\"Pre-training accuracy: %.4f%%\" % (100 * score[0]))\n",
    "    epochs = 5\n",
    "    start_time = datetime.now()\n",
    "    history = model.fit(train_loader, epochs=epochs)\n",
    "    end_time = datetime.now() - start_time\n",
    "    print(\"\\nTraining completed in time: {}\".format(end_time))\n",
    "    loss, eval = model.evaluate(test_loader)\n",
    "    wandb.log({\n",
    "        \"test_loss\": loss,\n",
    "        \"test_accuracy\": eval[0]\n",
    "    })\n",
    "\n",
    "    # Print the evaluation metrics\n",
    "    print(f\"testing accuracy: {eval[0] * 100:.2f}\")\n",
    "    print(f\"Balanced Accuracy: {eval[1] * 100:.2f}%\")\n",
    "    print(\"Classification Report:\\n\", eval[2])\n",
    "\n",
    "# %%\n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "    wandb.init(project=\"speech_bias\")\n",
    "\n",
    "    d = pd.read_csv(\"Data/TORGO/TORGO_info.csv\")\n",
    "    train_loader, test_loader = ProcessData(d, batch_size= 1024)\n",
    "    print(len(train_loader))\n",
    "\n",
    "    # Check if a GPU is available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Initialize the model\n",
    "    model = Net(device).to(device)\n",
    "\n",
    "    train(model, train_loader, test_loader)\n",
    "    \n",
    "    wandb.finish()\n",
    "# %%\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
